---
title: "430 Final"
author: "Dustin Oakes"
date: "12/17/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Analytical Questions

## Question 1

### (a) Influence vs. Leverage

Leverage refers to the tendency for certain predictor values to be further away relative to other values of the variable. Influence refers to how a particular data point impacts a regression fit. A point can have high leverage, and low influence if it sits in the trend line, but simply further away from most of the data. Similarly, a data point can have low leverage and high influence if the predictor values are close but the predicted values are spread out. Examining scatter plots of the data and looking at trends in the residuals can help to identify these concepts.

### (b) Missing Observations

There are a few methods to handle missing data points. Firstly, we could simply do our analysis without the missing data points. Another method, especially for use with categorical variables, would be to simply use the most frequent value. However, in time series analysis we might run into some problems with irregularly spaced data, etc, so it might be smart to try and interpolate what the values of the data should have been. By calculating the average or another statistic, we can fill in the data points and go about our analysis. We must be careful about our assumptions with imputation (mean, median, k-Nearest Neighbor), however, as we might make conclusions based on non-real data.

### (c) Bootstrap Coefficients

Computing bootstrap estimates of the regression coefficients can help determine how reliable and precise your estimate is. If the bootstrap estimates differ greatly, then it is likely the model is not very robust and will not hold up especially in terms of prediction/forecasting.

### (d) Regression Splines

Splines can be useful when the trend changes over time. If one functional form is not useful to explain the variation across all the data, it can be useful to split up the data into separate bins, and then use distinct models that work well for each part. Another good use for splines can be to better model time series, especially for historical data where today's models/paradigm do not at all apply to the situation at hand.

## Question 2

### (a) Hypothesis Test

Null Hypothesis: b2 = b3
Alternative: b2 =/ b3

```{r}
b1 = 0.36174
b2 = 0.43285
se = 0.06397
t = (b1-b2)/se
tc = qt(0.975, 352)
-tc < t
t < tc
```

Based on the hypothesis test, we cannot reject the null hypothesis, as the calculated test statistic does not fall above or below the critical value. So we cannot conclude that the two are not equal.

### (b) Joint Hypothesis Test

### (c) Omitting Variables

When FERT is omitted, naturally the model attributes more production to both AREA and LABOR, so both their coefficients increase slightly (by about 0.1). The standard errors stay virtually identical, and the r-squared decreases slightly.   
When LABOR is omitted, there is much more wight given to AREA, and a small increase for FERT. The AREA coefficient rises about 3 times more than the FERT coefficient, which suggests that between the two variables, adding land will produce more rice than adding fertilizer. Both coefficients standard errors decrease, but the r-squared does slightly as well.   
When AREA is left out of the model, it gives a similar response as to when LABOR was left out. The LABOR coefficient rises markedly, while the FERT coefficient rises just a little bit. Standard errors decrease and r-squared is still less than the original model. Overall, it appears that land and labor are the more influential variables when predicting total rice production.

## Question 4

### (a) Time Series

First things first, I would plot the time series to see if there were any immediately recognizable phenomena, such as mean-reversion, long-term trends, seasonality, or changing volatility. Next I would check ACF/PACF plots, to see if any obvious spikes would lead to forming an AR/MA model. A seasonal decomposition plot would also be helpful in determining what kind of model I would want to fit. Next, I would fit some models, and look at diagnostic statistics and selection criteria (r-squared, the various mean error measures, AIC/BIC, t-statistics for coefficients, etc.). Once I was happy with a model, I might do a forecast plot as a sanity check, just to make sure that everything seems to be going as expected. Finally, I would do some analysis on the selected model, looking at how the residuals behave, checking recursive residuals and the cumulative sum plot to make sure the model performs well throughout different time horizons.

## Question 5

### (a) Improving Model

Based on the correlograms, I would fit an ARMA model of around an order (2,3) or (3,4), depending on how we interpret the size of the spikes on the ACF/PACF. 

### (b) Covariance Stationarity

The data does not appear to be covariance stationary, as there is a clear downward trend over the decade. Covariance stationary data would have a mean-reverting trend, but clearly mortality has gone down as time goes by. Differencing the data set could help to make the data covariance stationary. 

### (c) Serial Correlation

There seems to be a negative serial correlation between the residuals. The residuals seem to change sign every time, flipping from positive to negative, and back to positive, on and on.

### (d) auto.arima

The auto.arima model suggested is an ARIMA model, with AR(2), I(1), and MA(1) components. The autoregressive part uses two lags of the data, while the integrated and moving average parts only look at one past data point. To account for seasonality, another ARMA model was fitted with an s-AR(2) and an s-MA(2) component, so both parts use two previous data measurements. Lastly, the 52 represents that we are analyzing weekly data. The first part of the model accounts for the long-term downward trend in the data, while the seasonal component accounts for the repeated yearly changes that are reliably predictable.

## Question 6

### (a) Chow Test

This test is examining whether the coefficients of the variables before 1940 are the same as the coefficients after 1940. Since the p-value is 0.02, at the 5% level we can reject the null hypothesis and conclude that there is in fact a difference in coefficients before and after 1940.

### (b) BP Test

This test is looking for heteroskedasticity. Since the p-value is not very small, we fail to reject the null hypothesis of homoskedasticity and cannot conclude that there is heteroskedasticity.

### (c) RESET Test

In the Ramsay RESET test, the null hypothesis is correct specification. For the first test (power=2), we get a small p-value, so we reject the null hypothesis and conclude that there is misspecification. For the second test (power=3), there is a larger p-value, so we fail to reject the null and conclude that the model does not suffer from misspecification.

## Question 7

### (a) Transforming Data

Taking the log of the series can help to minimize the influence of larger points at the end. In addition, fitting a GARCH model would be a good way to go about continuing our analysis of this series.

### (b) AR/MA

I would choose an AR model, mostly since the sales seem to bottom out at the same points, but the top points rise consistently throughout the data.

### (c) Trend, Season, Cycle

Out of the three components, I think seasonality and trend are the applicable ones. Seasonality is clear; there is less sales when nobody is at the beach. A long-term trend is also clear; the maximum level of sales rises consistently each year. I would not include a cyclical component as there are no identifiable long-term cycles such as a recession or war to be seen.

### (d) Seasonal Factors

The plot of seasonal factors shows us that across all years, sales are at their lowest at the beginning of the year, before rising to their maximum at the end of the year (indicating southern hemisphere, where people will go to the beach in December).

### (e) Improving Model Fit

The residuals exhibit more variation than we would like. This is visible on the ACF/PACF where certain spikes exceed the dashed bands. Perhaps fitting an ARIMA model would help to utilize some of the information in the lags, and therefore also minimize the volatility of the errors.

# Computational Questions

## Question 8

```{r}
library(AER)
data("PepperPrice")
```

### (a) TS Display

```{r}
library(forecast)
tsdisplay(PepperPrice[,1])
```

On the PACF, we see about 2 significant spikes, which would lead us to an AR(2) model. The ACF slowly trends towards zero, which tells us that many of the previous pepper prices contain information that helps predict the price at the next time horizon.

### (b) Additive Seasonal Adjustment

```{r}
#Decomposing with additive seasonality
decompose <- decompose(PepperPrice[,1], "additive")
#Making the seasonal adjustment
adjust_ts <- PepperPrice[,1] - decompose$seasonal
#Plotting the seasonally adjusted series
plot(PepperPrice[,1])
plot(adjust_ts)
```

Our seasonally adjusted plot has smoother curves than the original, as the repeated yearly trends are no longer shown.

### (c) Fitting Model

```{r}
black_full_model = Arima(adjust_ts, order=c(2,0,0))
summary(black_full_model)
```

Our fitted AR(2) model on our seasonally adjusted data returned a model which is on average about 4.6% off from predicting the correct black pepper price, according to the calculated MAPE. 

### (d) Forecasting

```{r}
plot(forecast(black_full_model, h=12))
```

Our forecast predicts a slight rise in the price of black pepper over the next year. However, the wide error bands make it somewhat complicated to draw any concrete conclusions from this forecast.

```{r}
library(forecast)
plot(forecast(ets(PepperPrice[,1]), h=12))
```


### (e) Multiplicative Seasonal Adjustment

```{r}
plot(stl(PepperPrice[,1], "per"))
```


Since the seasonal component does not change in magnitude over time, I think the additive adjustment is proper for this case.

### (f) Structural Test

```{r}
library(strucchange)
plot(efp(black_full_model$res~1, type = "Rec-CUSUM"))
```

The model seems to hold up reasonably well over time; there is no point where the significant bands are crossed.

```{r}
library(vars)
VARselect(PepperPrice, lag.max = 10)
```

Based on the VARselect function, we will fit a VAR(2) model to our time series.

```{r}
var_model=VAR(PepperPrice,p=2)
summary(var_model)
plot(var_model)
```

```{r}
grangertest(PepperPrice[,1] ~ PepperPrice[,2], order = 2)
```

```{r}
grangertest(PepperPrice[,2] ~ PepperPrice[,1], order = 2)
```

Our VAR model returns a high r-squared value of 0.98, however a few of the coefficients were not considered significant by the t-test. Our Granger Causality testing shows that the price of black pepper influences the price of white pepper, with strong significance. This makes a lot of sense when considering the different production processes of the two; producing white pepper requires more steps, so it is reasonable that changes in black pepper's price would appear in the marketplace before the same change in white pepper's price.

## Question 10

```{r}
library(PoEdata)
data("mroz", package="PoEdata")
Boruta <- Boruta::Boruta(wage ~ ., data=mroz)
plot(Boruta, las=2, cex.axis = 0.5)
```

Using the Boruta algorithm to select variables, 10 variables are suggested as good to use in the regression.

```{r}
vars_select <- c("hours","wage76","lfp","educ","mtr",
"faminc","exper","hwage","hhours","taxableinc","federaltax")
vars_select_data <- mroz[vars_select]
corrplot::corrplot(cor(vars_select_data))
```

Doing a correlation plot between our selected variables, we see high collinearity between taxable income and federal taxes, which is not a surprise. To avoid multicollinearity issues, we will omit federal tax from here on out.

```{r}
vars_select <- c("hours","wage76","lfp","educ","mtr",
"faminc","exper","hwage","hhours","taxableinc")
vars_select_data <- mroz[vars_select]
```

```{r}
wage_model = lm(wage ~ hours + wage76 + lfp + educ + mtr + faminc + exper + hwage + hhours + taxableinc, data = mroz)
summary(wage_model)
```

According to our first model fitted, experience an taxable income are not statistically significant predictors. Let's try another without these variables and see if it changes at all.

```{r}
wage_model.2 = lm(wage ~ hours + wage76 + lfp + educ + mtr + faminc + hwage + hhours, data = mroz)
summary(wage_model.2)
```

All our predictors are now considered to be significant at some level, but our adjusted r-squared did not really improve at all. 

```{r}
plot(wage_model.2)
```


```{r}
library(caret)
train.control <- trainControl(method="cv", number=5, savePredictions = TRUE, returnResamp = "all")
cvmodel <- train(wage ~ hours + wage76 + lfp + educ + mtr + faminc + hwage + hhours, data=mroz, method = "lm", trControl = train.control)
print(cvmodel)
```

Using a 5-Fold Cross Validation test to evaluate our out of sample performance shows that the model performs similarly with sampled data. The r-squared value is similar to what our original model reported. The RMSE of around 2 is not bad, but the r-squared is still somewhat low due to the largest outliers exercising influence.